{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: Titanic\n",
    "\n",
    "#### Copmetition Specification:\n",
    "The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n",
    "\n",
    "#### Approach:\n",
    "* Feature creation using pandas\n",
    "* Create pipeline to manage preprocessing (transformation) and stacking (model)\n",
    "* Use grid search to optimise pipelines parameters\n",
    "\n",
    "#### Objective is to demonstrate:\n",
    "* feature creation\n",
    "* sklearn pipelines\n",
    "* target encoding\n",
    "* XGBoost, Random Forest\n",
    "* Cross-validation\n",
    "* Performance metrics: accuracy, precision/recall, ROC\n",
    "* Stacking\n",
    "* Hyperparameter searching/tuning\n",
    "\n",
    "##### Limitations:\n",
    "* no eda: there are many other notebooks for Titanic competition with in depth eda to read\n",
    "\n",
    "##### Further Enhancements:\n",
    "* The groups in which people were travelling were a very important indicator of survival. Take a deeper look into what defines those group e.g. nanny-child groups, solo male/female travellers, other segments.\n",
    "* Manually label cabins to locate cabin with in a deck. Think of the location of cabin as a 3D coordinate. Currently you have two of the three coordinates. Manual labelling is the only way to access the third.\n",
    "* Think more carefully about how to define miss as adult vs child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/IainMac/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "# kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# file handling\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# data hnadling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "# classification models\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# encoding methods\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# sklearn pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# metrics to assess classification models\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# other imports\n",
    "from pprint import pprint\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "* graphs and words\n",
    "* for each deck encode the location of the cabin on an x/y plane\n",
    "* mayb interaction terms and feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "* XGBoost (summary): https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d\n",
    "* XGBoost (is feature scaling required?): https://www.quora.com/Why-is-it-not-a-good-idea-to-do-feature-scaling-for-xgboost\n",
    "* XGBoost (parameter tuning): https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "\n",
    "* skLearn pipeelines (introduction): https://www.kaggle.com/dansbecker/pipelines\n",
    "* skLearn pipelines (tutorial): https://www.kaggle.com/aashita/advanced-pipelines-tutorial\n",
    "* skLearn pipelines (examples): https://www.codementor.io/bruce3557/beautiful-machine-learning-pipeline-with-scikit-learn-uiqapbxuj\n",
    "* skLearn pipelines (custom transformers - for feature creation): https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65\n",
    "* sklearn pipelines (with XGBoost): https://stackoverflow.com/questions/42793709/how-to-optimize-a-sklearn-pipeline-using-xgboost-for-a-different-eval-metric\n",
    "\n",
    "\n",
    "* stacking api: https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/#methods\n",
    "* model stacking: https://datascience.stackexchange.com/questions/43436/stacking-doesnt-improve-accuracy\n",
    "\n",
    "* extracting feature names: https://stackoverflow.com/questions/35376293/extracting-selected-feature-names-from-scikit-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(dep_variable_name, df_train, df_test):\n",
    "    # move dependant variable to last column of frame\n",
    "    dep_variable = df_train[dep_variable_name]\n",
    "    df_train.drop(dep_variable_name, axis=1, inplace=True)\n",
    "    df_train = df_train.merge(dep_variable, left_index=True, right_index=True)\n",
    "\n",
    "    # split dependant and independant variables\n",
    "    X_train = df_train.to_numpy()[:,:-1]\n",
    "    y_train = df_train.to_numpy()[:,-1:].ravel()\n",
    "\n",
    "    X_test = df_test.to_numpy()\n",
    "    # y_test unknown: part of project submission\n",
    "    y_test = 0\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def create_submission_df(df_passengerID, df_pred):\n",
    "    df_submission = df_passengerID.merge(df_pred, left_index=True, right_index=True);\n",
    "\n",
    "    return df_submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/IainMac/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "save_folder = \"DataSets/\"\n",
    "\n",
    "# if save folder does not exist, create\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    \n",
    "# authenticate and download competition\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "competition = \"titanic\"\n",
    "files = api.competition_download_files(competition, path = save_folder)\n",
    "\n",
    "# unzip files and remove zip file\n",
    "dataSet_folder = save_folder + competition\n",
    "\n",
    "with zipfile.ZipFile(dataSet_folder + \".zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(dataSet_folder)\n",
    "    \n",
    "os.remove(dataSet_folder + \".zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"DataSets/\"\n",
    "competition = \"titanic\"\n",
    "dataSet_folder = save_folder + competition\n",
    "\n",
    "train_csv = 'train.csv'\n",
    "test_csv = 'test.csv'\n",
    "\n",
    "trainData = os.path.join(dataSet_folder, train_csv)\n",
    "testData = os.path.join(dataSet_folder, test_csv)\n",
    "\n",
    "df_train = pd.read_csv(trainData)\n",
    "df_test = pd.read_csv(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataFrame to be used later for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_passengerID = pd.DataFrame(df_test['PassengerId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataFrames so all preprocessing can be done on one dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge DataFrames\n",
    "df_test['Survived'] = np.NaN\n",
    "df_test['Test/Train'] = 'Test'\n",
    "df_train['Test/Train'] = 'Train'\n",
    "df_all = pd.concat([df_train.copy(), df_test.copy()], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age\n",
    "\n",
    "Impute moved to after Title Group is extracted to get a more accurate imput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_embark = df_all['Embarked'].value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Embarked'] = df_all['Embarked'].fillna(mode_embark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_Fare = df_all['Fare'].mean(skipna = True)\n",
    "df_all['Fare'] = df_all['Fare'].fillna(mean_Fare,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "We will look deeper into the dataset to see if we can create new features to provide further insight and improve the accuracy score of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where were the idividuals at the time of the crash?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deck\n",
    "\n",
    "There were 10 decks in total. From top to bottom they were the Boat Deck, the Promenade Deck (deck A), passenger decks B to G, Orlop Deck, and the Tank Top. Orlop deck is below deck G and Tank Top is below the Orlop Deck.\n",
    "\n",
    "There were no passangers on the Orlop or Tank Top decks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Deck'] = df_all['Cabin']\n",
    "df_all['Deck'] = df_all['Deck'].fillna('Z');\n",
    "df_all['Deck'] = df_all['Deck'].str[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vertical Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all['Vertical_Location'] = df_all['Cabin']\n",
    "df_all['Vertical_Location'] = df_all['Cabin'].fillna('Z00');\n",
    "df_all['Vertical_Location'] = df_all['Vertical_Location'].str[1:]\n",
    "df_all['Vertical_Location'].replace('23 C25 C27', '23', inplace=True)\n",
    "df_all['Vertical_Location'].replace(' G73', '73', inplace=True)\n",
    "df_all['Vertical_Location'].replace('10 D12', '10', inplace=True)\n",
    "df_all['Vertical_Location'].replace('58 B60', '58', inplace=True)\n",
    "df_all['Vertical_Location'].replace(' E69', '69', inplace=True) \n",
    "df_all['Vertical_Location'].replace('22 C26', '22', inplace=True)\n",
    "df_all['Vertical_Location'].replace('57 B59 B63 B66', '57', inplace=True)\n",
    "df_all['Vertical_Location'].replace('96 B98', '96', inplace=True) \n",
    "df_all['Vertical_Location'].replace('51 B53 B55', '51', inplace=True) \n",
    "df_all['Vertical_Location'].replace(' G63', '63', inplace=True) \n",
    "df_all['Vertical_Location'].replace('62 C64', '62', inplace=True)\n",
    "df_all['Vertical_Location'].replace('82 B84', '82', inplace=True) \n",
    "df_all['Vertical_Location'].replace('55 C57', '55', inplace=True) \n",
    "df_all['Vertical_Location'].replace('39 E41', '39', inplace=True) \n",
    "df_all['Vertical_Location'].replace('52 B54 B56', '52', inplace=True)\n",
    "df_all['Vertical_Location'].replace(' E46', '46', inplace=True)\n",
    "df_all['Vertical_Location'].replace(' E57', '57', inplace=True) \n",
    "df_all['Vertical_Location'] = pd.to_numeric(df_all['Vertical_Location'])\n",
    "\n",
    "def title_group_conditions(x):\n",
    "    if x['Deck'] in ['Z','T']:\n",
    "        return 'unknown'\n",
    "    if (x['Deck'] not in ['Z','T']) and (x['Vertical_Location']%2==0.0) and (x['Deck'] in ['A','B','C','D']):\n",
    "        return 'Top'\n",
    "    else:\n",
    "        return 'Bottom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Vertical_Location'] = df_all.apply(title_group_conditions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What further information can we extract about the idividual?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['Surname'] = df_all.Name.str.extract('([A-Za-z]+),')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ... to extract title from name\n",
    "df_all['Title'] = df_all.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace French Titles with English Equivalents\n",
    "df_all['Title'] = df_all['Title'].replace('Mlle', 'Miss')\n",
    "df_all['Title'] = df_all['Title'].replace('Ms', 'Miss')\n",
    "df_all['Title'] = df_all['Title'].replace('Mme', 'Mrs')\n",
    "df_all['Title'] = df_all['Title'].replace('Ms', 'Miss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_group_conditions(x):\n",
    "    if ((x['Title'] not in ['Mr', 'Miss', 'Mrs', 'Master'] ) and (x['Sex']=='male')):\n",
    "        return 'Mr'\n",
    "    elif ((x['Title'] not in ['Mr', 'Miss', 'Mrs', 'Master'] ) and (x['Sex']=='female')):\n",
    "        return 'Mrs'\n",
    "    else:\n",
    "        return x['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Title Group'] = df_all.apply(title_group_conditions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Impute Ages Based on Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/IainMac/anaconda3/envs/portfolio_machinelearning/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n",
      "/Users/IainMac/anaconda3/envs/portfolio_machinelearning/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/IainMac/anaconda3/envs/portfolio_machinelearning/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  app.launch_new_instance()\n",
      "/Users/IainMac/anaconda3/envs/portfolio_machinelearning/lib/python3.7/site-packages/ipykernel_launcher.py:17: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "mean_Age = df_all['Age'].mean(skipna = True)\n",
    "df_all['Age'] = df_all['Age'].fillna(mean_Age,)\n",
    "\n",
    "mr_mean_age = df_all[df_all['Title Group'] == 'Mr']['Age'].mean()\n",
    "master_mean_age = df_all[df_all['Title Group'] == 'Master']['Age'].mean()\n",
    "mrs_mean_age = df_all[df_all['Title Group'] == 'Mrs']['Age'].mean()\n",
    "miss_mean_age = df_all[df_all['Title Group'] == 'Miss']['Age'].mean()\n",
    "\n",
    "mr_index = list(df_all[(df_all['Age'].isna()) & (df_all['Title Group'] == 'Mr')]['Age'].index)\n",
    "master_index = list(df_all[(df_all['Age'].isna()) & (df_all['Title Group'] == 'Master')]['Age'].index)\n",
    "mrs_index = list(df_all[(df_all['Age'].isna()) & (df_all['Title Group'] == 'Mrs')]['Age'].index)\n",
    "miss_idnex = list(df_all[(df_all['Age'].isna()) & (df_all['Title Group'] == 'Miss')]['Age'].index)\n",
    "\n",
    "df_all.set_value(mr_index, 'Age', mr_mean_age);\n",
    "df_all.set_value(master_index, 'Age', master_mean_age);\n",
    "df_all.set_value(mrs_index, 'Age', mrs_mean_age);\n",
    "df_all.set_value(miss_idnex, 'Age', miss_mean_age);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Famliy Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Family Size'] = df_all['SibSp'] + df_all['Parch'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### is Boy Man or Female ?\n",
    "Women and children. Women encompasses both children and adults. The distinction that needs to be drawn here is what defines an adult vs child boy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_boy_man_female_conditions(x):\n",
    "    if ((x['Title Group'] in ['Master']) and (x['Sex'] == 'male')):\n",
    "        return 'boy'\n",
    "    elif ((x['Title Group'] in ['Mr']) and (x['Sex'] == 'male')):\n",
    "        return 'man'\n",
    "    else:\n",
    "        return 'female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['boy_man_female'] = df_all.apply(is_boy_man_female_conditions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### isParent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isadult_conditions(x):\n",
    "    if ((x['Title Group'] in ['Mr','Mrs'])):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['isAdult'] = df_all.apply(isadult_conditions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### isChild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ischild_conditions(x):\n",
    "    if ((x['Title Group'] in ['Master','Miss'])):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['isChild'] = df_all.apply(ischild_conditions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fareBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Bins\n",
    "df_all['FareBin'] = pd.qcut(df_all['Fare'], 5).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ageBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Bins\n",
    "df_all['AgeBin'] = pd.qcut(df_all['Age'], 5).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What group was the person travelling with and what information can we obtain about these groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FamilyGroupID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Surname+Ticket Count\n",
    "check_shape = df_all.shape\n",
    "\n",
    "df_all['FamilyGroup_ID'] = df_all['Surname'] +  df_all['Ticket'] + df_all['Embarked'] + df_all['Pclass'].astype(str) + df_all['Fare'].astype(str)\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ticket Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4, 3, 7, 5, 6, 0])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_shape = df_all.shape\n",
    "\n",
    "df_temp = df_all.groupby('Ticket').count()['Survived'].reset_index()\n",
    "df_temp.rename(columns={'Survived':'Ticket_Count'}, inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='Ticket',how='left')\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)\n",
    "\n",
    "df_all['Ticket_Count'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FamilyID Count (excluding self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  3,  2,  6,  4,  5, -1])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_shape = df_all.shape\n",
    "\n",
    "df_temp = df_all.groupby('FamilyGroup_ID').count()['Survived'].reset_index()\n",
    "df_temp.rename(columns={'Survived':'FamilyGroup_ID_Count'}, inplace=True)\n",
    "df_temp['FamilyGroup_ID_Count'] = df_temp['FamilyGroup_ID_Count'] - 1\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)\n",
    "\n",
    "df_all['FamilyGroup_ID_Count'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FamilyID Child Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_shape = df_all.shape\n",
    "\n",
    "df_temp = df_all.groupby('FamilyGroup_ID').sum()['isChild'].reset_index()\n",
    "df_temp.rename(columns={'isChild':'FamilyGroup_Child_Count'}, inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FamilyID Adult Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncheck_shape = df_all.shape\\n\\ndf_temp = df_all.groupby('FamilyGroup_ID').sum()['isAdult'].reset_index()\\ndf_temp.rename(columns={'isAdult':'FamilyGroup_Parent_Count'}, inplace=True)\\ndf_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\\n\\nassert((df_all.shape[1] - check_shape[1])==1)\\n\""
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "check_shape = df_all.shape\n",
    "\n",
    "df_temp = df_all.groupby('FamilyGroup_ID').sum()['isAdult'].reset_index()\n",
    "df_temp.rename(columns={'isAdult':'FamilyGroup_Parent_Count'}, inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FamilyID Female Adult Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  0.])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_shape = df_all.shape\n",
    "\n",
    "df_temp = df_all.groupby(['FamilyGroup_ID','Sex']).sum()['isAdult'].reset_index()\n",
    "df_temp = df_temp[df_temp['Sex']=='female']\n",
    "df_temp.rename(columns={'isAdult':'FamilyGroup_FemaleAdult_Count'}, inplace=True)\n",
    "df_temp.drop('Sex',axis=1 ,inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n",
    "df_all['FamilyGroup_FemaleAdult_Count'].fillna(value=-1, inplace=True)\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)\n",
    "df_all['FamilyGroup_FemaleAdult_Count'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FamilyID Male Adult Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  0.,  2.,  3.,  4.])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_shape = df_all.shape\n",
    "\n",
    "df_temp = df_all.groupby(['FamilyGroup_ID','Sex']).sum()['isAdult'].reset_index()\n",
    "df_temp = df_temp[df_temp['Sex']=='male']\n",
    "df_temp.rename(columns={'isAdult':'FamilyGroup_MaleAdult_Count'}, inplace=True)\n",
    "df_temp.drop('Sex',axis=1 ,inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n",
    "df_all['FamilyGroup_MaleAdult_Count'].fillna(value=-1, inplace=True)\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)\n",
    "df_all['FamilyGroup_MaleAdult_Count'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FramilyGroupID Survival Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of family IDs in both test and train set\n",
    "set_test = set(df_all[df_all['Test/Train']=='Test']['FamilyGroup_ID'].unique())\n",
    "set_train = set(df_all[df_all['Test/Train']=='Train']['FamilyGroup_ID'].unique())\n",
    "family_id_both = set_test.intersection(set_train)\n",
    "\n",
    "##### list of family IDs in only train set\n",
    "family_id_train = set_train - set_test\n",
    "\n",
    "##### list of family IDs in only test set\n",
    "family_id_test = set_test - set_train\n",
    "\n",
    "assert(family_id_train.intersection(family_id_both) == set())\n",
    "assert(family_id_test.intersection(family_id_both) == set())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### survival count\n",
    "check_shape = df_all.shape\n",
    "\n",
    "df_temp = df_all.groupby('FamilyGroup_ID').sum()['Survived'].reset_index()\n",
    "df_temp.rename(columns={'Survived':'FamilyGroup_SurvivalCount'}, inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n",
    "\n",
    "# create survived temp and fill nan with 0 (this is so when we deduct \n",
    "# indvidiual survival count from group survival count we don't return a nan)\n",
    "df_all['Survived_temp'] = df_all['Survived']\n",
    "df_all['Survived_temp'].fillna(value=0, inplace=True)\n",
    "\n",
    "df_all['FamilyGroup_SurvivalCount'] = df_all['FamilyGroup_SurvivalCount'] - df_all['Survived_temp']\n",
    "\n",
    "df_all.drop('Survived_temp', axis=1, inplace=True)\n",
    "\n",
    "assert((df_all.shape[1] - check_shape[1])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family_group_id_sizes == 1 do not encode anything as you are inputing the target variable into the model as a\n",
    "# feature, this will cause massive overfitting\n",
    "\n",
    "# family_group_id_survival_count - individual survival count (same as above)\n",
    "\n",
    "## family sizes 1\n",
    "# G - family group sizes exactly 1\n",
    "\n",
    "## whole group survival information known\n",
    "# A - exactly 1\n",
    "# B - exactly 0\n",
    "# C - whole group survives\n",
    "\n",
    "## not whole group information known\n",
    "# D - at least 1 (at least 1 survives known with some unknowns)\n",
    "# E - maybe 1 (0 survived with unkowns)\n",
    "# F - unknown (all other family members survival not known)\n",
    "\n",
    "def group_survival_status(x):\n",
    "    \n",
    "    if x['FamilyGroup_ID_Count'] == 0:\n",
    "        return 'G'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID_Count']!=0) and (x['FamilyGroup_ID'] in family_id_train) and \n",
    "          (x['FamilyGroup_SurvivalCount'] == 1)):\n",
    "    #family survival known and #family survival == 1:\n",
    "        return 'A'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID_Count']!=0) and (x['FamilyGroup_ID'] in family_id_train) and \n",
    "          (x['FamilyGroup_SurvivalCount'] == 0)):\n",
    "    #family survival known and #family survival == 0:\n",
    "        return 'B'\n",
    "\n",
    "    #elif ((x['FamilyGroup_ID_Count']!=0) and (x['FamilyGroup_ID'] in family_id_train) and \n",
    "          #(x['FamilyGroup_SurvivalCount'] == x['FamilyGroup_ID_Count'])):\n",
    "    #family survival known and #family survival == family size:\n",
    "        #return 'C'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID'] in family_id_both) and (x['FamilyGroup_ID_Count']!=0) and \n",
    "          (x['FamilyGroup_SurvivalCount'] > 0)):\n",
    "    #family survival not known fully #at least one survives:\n",
    "        return 'D'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID'] in family_id_both) and (x['FamilyGroup_ID_Count']!=0) and \n",
    "          (x['FamilyGroup_SurvivalCount'] == 0)):\n",
    "    #family survival not known and #zero survived:\n",
    "        return 'E'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID'] in family_id_test) and (x['FamilyGroup_ID_Count']!=0)):\n",
    "    #family survival not known \n",
    "        return 'F'\n",
    "\n",
    "    else:\n",
    "    # just encase\n",
    "        return 'I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['FamilyID_Survival_Group'] = df_all.apply(group_survival_status, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.drop('FamilyGroup_SurvivalCount', axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Woman and Child Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_woman_and_child_group_conditions(x):\n",
    "    if x['FamilyGroup_FemaleAdult_Count']>0 and x['FamilyGroup_Child_Count']>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['isWomanAndChild'] = df_all.apply(is_woman_and_child_group_conditions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Woman and Child Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['womanAndChildCount'] = df_all['FamilyGroup_Child_Count'] + df_all['FamilyGroup_FemaleAdult_Count'] - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Woman and Child Survival Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_temp = df_all.groupby(['FamilyGroup_ID','isChild']).sum()['Survived'].reset_index()\n",
    "df_temp = df_temp[df_temp['isChild']==1]\n",
    "df_temp.drop('isChild', axis=1, inplace=True)\n",
    "df_temp.rename(columns={'Survived':'FamilyGroup_ChildSurvivalCount'}, inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_temp = df_all.groupby(['FamilyGroup_ID','isAdult', 'Sex']).sum()['Survived'].reset_index()\n",
    "df_temp = df_temp[df_temp['isAdult']==1]\n",
    "df_temp = df_temp[df_temp['Sex']=='female']\n",
    "df_temp.drop(['isAdult','Sex'], axis=1, inplace=True)\n",
    "df_temp.rename(columns={'Survived':'FamilyGroup_FemaleSurvivalCount'}, inplace=True)\n",
    "df_all = df_all.merge(df_temp, on='FamilyGroup_ID',how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Survived_temp'] = df_all['Survived']\n",
    "df_all['Survived_temp'].fillna(value=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['womanAndChildSurvivalRatio'] = ((df_all['FamilyGroup_ChildSurvivalCount'] + \n",
    "                                        df_all['FamilyGroup_FemaleSurvivalCount'] - df_all['Survived_temp']) / \n",
    "                                        df_all['womanAndChildCount'])\n",
    "\n",
    "df_all['womanAndChildSurvivalCount'] = ((df_all['FamilyGroup_ChildSurvivalCount'] + \n",
    "                                        df_all['FamilyGroup_FemaleSurvivalCount'] - df_all['Survived_temp'])) \n",
    "\n",
    "\n",
    "df_all.drop('Survived_temp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_survival_status(x):\n",
    "    \n",
    "    if x['isWomanAndChild'] == 0:\n",
    "        return 'Z'\n",
    "\n",
    "    elif ((x['isWomanAndChild']!=0) and (x['FamilyGroup_ID'] in family_id_train) and \n",
    "          (x['womanAndChildSurvivalRatio'] == 1)):\n",
    "    #family survival known and #family survival == 1:\n",
    "        return 'A'\n",
    "\n",
    "    elif ((x['isWomanAndChild']!=0) and (x['FamilyGroup_ID'] in family_id_train) and \n",
    "          (x['womanAndChildSurvivalRatio'] == 0)):\n",
    "    #family survival known and #family survival == 0:\n",
    "        return 'B'\n",
    "\n",
    "    #elif ((x['FamilyGroup_ID_Count']!=0) and (x['FamilyGroup_ID'] in family_id_train) and \n",
    "          #(x['FamilyGroup_SurvivalCount'] == x['FamilyGroup_ID_Count'])):\n",
    "    #family survival known and #family survival == family size:\n",
    "        #return 'C'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID'] in family_id_both) and (x['isWomanAndChild']!=0) and \n",
    "          (x['womanAndChildSurvivalCount'] > 0)):\n",
    "    #family survival not known fully #at least one survives:\n",
    "        return 'D'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID'] in family_id_both) and (x['isWomanAndChild']!=0) and \n",
    "          (x['womanAndChildSurvivalCount'] == 0)):\n",
    "    #family survival not known and #zero survived:\n",
    "        return 'E'\n",
    "\n",
    "    elif ((x['FamilyGroup_ID'] in family_id_test) and (x['isWomanAndChild']!=0)):\n",
    "    #family survival not known \n",
    "        return 'F'\n",
    "\n",
    "    else:\n",
    "    # just encase\n",
    "        return 'I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['womanAndChild_Survival'] = df_all.apply(group_survival_status, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.drop('womanAndChildSurvivalRatio', axis=1, inplace=True)\n",
    "df_all.drop('womanAndChildSurvivalCount', axis=1, inplace=True)\n",
    "df_all.drop('FamilyGroup_ChildSurvivalCount', axis=1, inplace=True)\n",
    "df_all.drop('FamilyGroup_FemaleSurvivalCount', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Name', 'Cabin', 'PassengerId', 'Ticket', 'FamilyGroup_ID', 'Surname', 'Title', 'Age', 'Fare', 'isAdult']\n",
    "df_all.drop(drop_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate Train and Test DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split DataFrames\n",
    "df_train = df_all[df_all['Test/Train']=='Train'].copy()\n",
    "df_test = df_all[df_all['Test/Train']=='Test'].copy()\n",
    "df_train.drop(['Test/Train'], axis=1, inplace=True)\n",
    "df_test.drop(['Test/Train','Survived'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop('Survived', axis=1)\n",
    "y_train = df_train['Survived']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Feat = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = []\n",
    "cat_feat = list(set(X_train.columns.tolist()) - set(num_feat))\n",
    "\n",
    "assert (set(num_feat).union(set(cat_feat)) == set(all_Feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Component for Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comp = (\"numerical_features\", ColumnTransformer([\n",
    "                (\"numerical\", Pipeline(steps=[(\n",
    "                    \"impute_stage\", SimpleImputer(missing_values=np.nan, strategy=\"mean\"))]), num_feat\n",
    "                )])\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Component for  Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_comp = (\"categorical_features\", ColumnTransformer([\n",
    "                (\"categorical_mode\", Pipeline(steps=[\n",
    "                    (\"impute_stage\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "                    (\"label_encoder\", ce.TargetEncoder(handle_unknown=\"impute\"))]), cat_feat \n",
    "                )])\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Component for Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_comp = (\"features\", FeatureUnion([num_comp, cat_comp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp_xgb = (\"classifiers\", XGBClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Full Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_xgb  = Pipeline(steps=[trans_comp, model_comp_xgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pipeline - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_xgb.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy score: 0.8329370668455175\n",
      "CV Precision score: 0.8172616391473015\n",
      "CV Recall score: 0.735978835978836\n",
      "CV ROC score: 0.8794676764880253\n"
     ]
    }
   ],
   "source": [
    "scores_accuracy = cross_val_score(full_pipeline_xgb, X_train, y_train, cv=10, scoring = \"accuracy\")\n",
    "scores_precision = cross_val_score(full_pipeline_xgb, X_train, y_train, cv=10, scoring = \"precision\")\n",
    "scores_recall = cross_val_score(full_pipeline_xgb, X_train, y_train, cv=10, scoring = \"recall\")\n",
    "scores_rocauc = cross_val_score(full_pipeline_xgb, X_train, y_train, cv=10, scoring = \"roc_auc\")\n",
    "\n",
    "print(\"CV Accuracy score: \" + str(scores_accuracy.mean()))\n",
    "print(\"CV Precision score: \" + str(scores_precision.mean()))\n",
    "print(\"CV Recall score: \" + str(scores_recall.mean()))\n",
    "print(\"CV ROC score: \" + str(scores_rocauc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = full_pipeline_xgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score: 0.8379888268156425\n",
      "Test Precision score: 0.8125\n",
      "Test Recall score: 0.7536231884057971\n",
      "Test ROC score: 0.822266139657444\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy score: \" + str(accuracy_score(y_val,y_pred)))\n",
    "print(\"Test Precision score: \" + str(precision_score(y_val,y_pred)))\n",
    "print(\"Test Recall score: \" + str(recall_score(y_val,y_pred)))\n",
    "print(\"Test ROC score: \" + str(roc_auc_score(y_val,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "womanAndChild_Survival has importances: 51.37%\n",
      "womanAndChildCount has importances: 7.89%\n",
      "isWomanAndChild has importances: 7.12%\n",
      "FamilyID_Survival_Group has importances: 4.99%\n",
      "FamilyGroup_MaleAdult_Count has importances: 3.71%\n",
      "FamilyGroup_FemaleAdult_Count has importances: 3.13%\n",
      "FamilyGroup_Child_Count has importances: 2.97%\n",
      "FamilyGroup_ID_Count has importances: 2.38%\n",
      "Ticket_Count has importances: 2.08%\n",
      "AgeBin has importances: 1.8%\n",
      "FareBin has importances: 1.73%\n",
      "isChild has importances: 1.53%\n",
      "isAdult has importances: 1.52%\n",
      "Family Size has importances: 1.35%\n",
      "Title Group has importances: 1.28%\n",
      "Vertical_Location has importances: 1.19%\n",
      "Deck has importances: 1.17%\n",
      "SibSp has importances: 1.16%\n",
      "Sex has importances: 1.02%\n",
      "Pclass has importances: 0.59%\n",
      "Parch has importances: 0.0%\n",
      "Embarked has importances: 0.0%\n"
     ]
    }
   ],
   "source": [
    "model = full_pipeline_xgb.steps[1][1]\n",
    "selection_xgb = SelectFromModel(model, threshold=0, prefit=True)\n",
    "feature_names_example = X_train.columns.to_numpy().reshape(1, -1)\n",
    "selected_features = selection_xgb.transform(feature_names_example)\n",
    "selected_features = list(selected_features[0])\n",
    "selected_features.reverse()\n",
    "\n",
    "importances = list(full_pipeline_xgb.named_steps['classifiers'].feature_importances_)\n",
    "importances.sort()\n",
    "importances.reverse()\n",
    "\n",
    "for i in range(len(importances)):\n",
    "    print(selected_features[i] + ' has importances: ' + str(round(importances[i] * 100,2)) +'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Pipeline - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__max_depth': list(range(2,4,1)),\n",
    "             'classifiers__min_child_weight': list(range(0,2,1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8342696629213483,\n",
       " {'classifiers__max_depth': 2, 'classifiers__min_child_weight': 0})"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline_xgb, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_params= {'classifiers__max_depth': [2],\n",
    "             'classifiers__min_child_weight': [0],\n",
    "             'classifiers__gamma': [i/10.0 for i in range(0,5)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8342696629213483,\n",
       " {'classifiers__gamma': 0.0,\n",
       "  'classifiers__max_depth': 2,\n",
       "  'classifiers__min_child_weight': 0})"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline_xgb, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__max_depth': [2],\n",
    "             'classifiers__min_child_weight': [0],\n",
    "             'classifiers__gamma': [0.0],\n",
    "             'classifiers__colsample_bytree': [i/10.0 for i in range(8,11)],\n",
    "             'classifiers__subsample': [i/10.0 for i in range(8,11)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8342696629213483,\n",
       " {'classifiers__colsample_bytree': 1.0,\n",
       "  'classifiers__gamma': 0.0,\n",
       "  'classifiers__max_depth': 2,\n",
       "  'classifiers__min_child_weight': 0,\n",
       "  'classifiers__subsample': 1.0})"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline_xgb, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__max_depth': [2],\n",
    "             'classifiers__min_child_weight': [0],\n",
    "             'classifiers__gamma': [0],\n",
    "             'classifiers__colsample_bytree': [1],\n",
    "             'classifiers__subsample': [1],\n",
    "             'classifiers__reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8356741573033708,\n",
       " {'classifiers__colsample_bytree': 1,\n",
       "  'classifiers__gamma': 0,\n",
       "  'classifiers__max_depth': 2,\n",
       "  'classifiers__min_child_weight': 0,\n",
       "  'classifiers__reg_alpha': 1,\n",
       "  'classifiers__subsample': 1})"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline_xgb, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score: 0.8324022346368715\n",
      "Test Precision score: 0.8\n",
      "Test Recall score: 0.7536231884057971\n",
      "Test ROC score: 0.8177206851119895\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_searchCV.predict(X_val)\n",
    "\n",
    "print(\"Test Accuracy score: \" + str(accuracy_score(y_val,y_pred)))\n",
    "print(\"Test Precision score: \" + str(precision_score(y_val,y_pred)))\n",
    "print(\"Test Recall score: \" + str(recall_score(y_val,y_pred)))\n",
    "print(\"Test ROC score: \" + str(roc_auc_score(y_val,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model on Full Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop('Survived', axis=1)\n",
    "y_train = df_train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8473625140291807,\n",
       " {'classifiers__colsample_bytree': 1,\n",
       "  'classifiers__gamma': 0,\n",
       "  'classifiers__max_depth': 2,\n",
       "  'classifiers__min_child_weight': 0,\n",
       "  'classifiers__reg_alpha': 1,\n",
       "  'classifiers__subsample': 1})"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV.fit(X_train, y_train);\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test\n",
    "y_pred = grid_searchCV.predict(X_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.Series(y_pred, name='Survived')\n",
    "df_submission = create_submission_df(df_passengerID, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"/Users/IainMac/Desktop/Learning/01 Portfolio/02 Machine Learning/02 Classification/00 Kaggle Submissions/\"\n",
    "save_name = 'submission.csv'\n",
    "df_submission.to_csv(path_or_buf = save_folder + save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.77k/2.77k [00:12<00:00, 223B/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to Titanic: Machine Learning from Disaster"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.competition_submit(save_folder + save_name, message='skLearn Pipeline with XGBoost: Submission' ,competition=competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pipeline - Stacked\n",
    "\n",
    "No submissions were made using this pipeline, this is simply to show how to build a stacked model using sklearn pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "xgb = XGBClassifier(random_state=0)\n",
    "rfr = RandomForestClassifier(random_state=0, n_estimators=100)\n",
    "lgr = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=0)\n",
    "mlgr = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[xgb, gnb, knn], meta_classifier=mlgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comp = (\"classifiers\", sclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline(steps=[trans_comp, model_comp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy score: 0.8314883746926002\n",
      "CV Precision score: 0.8089678724851138\n",
      "CV Recall score: 0.7433862433862434\n",
      "CV ROC score: 0.8429136324485162\n"
     ]
    }
   ],
   "source": [
    "scores_accuracy = cross_val_score(full_pipeline, X_train, y_train, cv=10, scoring = \"accuracy\")\n",
    "scores_precision = cross_val_score(full_pipeline, X_train, y_train, cv=10, scoring = \"precision\")\n",
    "scores_recall = cross_val_score(full_pipeline, X_train, y_train, cv=10, scoring = \"recall\")\n",
    "scores_rocauc = cross_val_score(full_pipeline, X_train, y_train, cv=10, scoring = \"roc_auc\")\n",
    "\n",
    "print(\"CV Accuracy score: \" + str(scores_accuracy.mean()))\n",
    "print(\"CV Precision score: \" + str(scores_precision.mean()))\n",
    "print(\"CV Recall score: \" + str(scores_recall.mean()))\n",
    "print(\"CV ROC score: \" + str(scores_rocauc.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = full_pipeline.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score: 0.8212290502793296\n",
      "Test Precision score: 0.7936507936507936\n",
      "Test Recall score: 0.7246376811594203\n",
      "Test ROC score: 0.8032279314888011\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy score: \" + str(accuracy_score(y_val,y_pred)))\n",
    "print(\"Test Precision score: \" + str(precision_score(y_val,y_pred)))\n",
    "print(\"Test Recall score: \" + str(recall_score(y_val,y_pred)))\n",
    "print(\"Test ROC score: \" + str(roc_auc_score(y_val,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Pipeline - Stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Pipeline - RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Pipeline - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__randomforestclassifier__n_estimators':[30],\n",
    "              'classifiers__randomforestclassifier__min_samples_split': [2],\n",
    "              'classifiers__randomforestclassifier__min_samples_leaf': [3],\n",
    "              'classifiers__randomforestclassifier__max_features': ['auto'],\n",
    "              'classifiers__randomforestclassifier__max_depth':[10],\n",
    "             'classifiers__xgbclassifier__max_depth': list(range(2,4,1)),\n",
    "             'classifiers__xgbclassifier__min_child_weight': list(range(0,2,1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8342696629213483,\n",
       " {'classifiers__randomforestclassifier__max_depth': 10,\n",
       "  'classifiers__randomforestclassifier__max_features': 'auto',\n",
       "  'classifiers__randomforestclassifier__min_samples_leaf': 3,\n",
       "  'classifiers__randomforestclassifier__min_samples_split': 2,\n",
       "  'classifiers__randomforestclassifier__n_estimators': 30,\n",
       "  'classifiers__xgbclassifier__max_depth': 3,\n",
       "  'classifiers__xgbclassifier__min_child_weight': 1})"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_params= {'classifiers__randomforestclassifier__n_estimators':[30],\n",
    "              'classifiers__randomforestclassifier__min_samples_split': [2],\n",
    "              'classifiers__randomforestclassifier__min_samples_leaf': [3],\n",
    "              'classifiers__randomforestclassifier__max_features': ['auto'],\n",
    "              'classifiers__randomforestclassifier__max_depth':[10],\n",
    "             'classifiers__xgbclassifier__max_depth': [3],\n",
    "             'classifiers__xgbclassifier__min_child_weight': [1],\n",
    "             'classifiers__xgbclassifier__gamma': [i/10.0 for i in range(0,5)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8370786516853933,\n",
       " {'classifiers__randomforestclassifier__max_depth': 10,\n",
       "  'classifiers__randomforestclassifier__max_features': 'auto',\n",
       "  'classifiers__randomforestclassifier__min_samples_leaf': 3,\n",
       "  'classifiers__randomforestclassifier__min_samples_split': 2,\n",
       "  'classifiers__randomforestclassifier__n_estimators': 30,\n",
       "  'classifiers__xgbclassifier__gamma': 0.1,\n",
       "  'classifiers__xgbclassifier__max_depth': 3,\n",
       "  'classifiers__xgbclassifier__min_child_weight': 1})"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__randomforestclassifier__n_estimators':[30],\n",
    "              'classifiers__randomforestclassifier__min_samples_split': [2],\n",
    "              'classifiers__randomforestclassifier__min_samples_leaf': [3],\n",
    "              'classifiers__randomforestclassifier__max_features': ['auto'],\n",
    "              'classifiers__randomforestclassifier__max_depth':[10],\n",
    "             'classifiers__xgbclassifier__max_depth': [3],\n",
    "             'classifiers__xgbclassifier__min_child_weight': [1],\n",
    "             'classifiers__xgbclassifier__gamma': [0.1],\n",
    "             'classifiers__xgbclassifier__colsample_bytree': [i/10.0 for i in range(6,10)],\n",
    "             'classifiers__xgbclassifier__subsample': [i/10.0 for i in range(6,10)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8398876404494382,\n",
       " {'classifiers__randomforestclassifier__max_depth': 10,\n",
       "  'classifiers__randomforestclassifier__max_features': 'auto',\n",
       "  'classifiers__randomforestclassifier__min_samples_leaf': 3,\n",
       "  'classifiers__randomforestclassifier__min_samples_split': 2,\n",
       "  'classifiers__randomforestclassifier__n_estimators': 30,\n",
       "  'classifiers__xgbclassifier__colsample_bytree': 0.7,\n",
       "  'classifiers__xgbclassifier__gamma': 0.1,\n",
       "  'classifiers__xgbclassifier__max_depth': 3,\n",
       "  'classifiers__xgbclassifier__min_child_weight': 1,\n",
       "  'classifiers__xgbclassifier__subsample': 0.6})"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__randomforestclassifier__n_estimators':[30],\n",
    "              'classifiers__randomforestclassifier__min_samples_split': [2],\n",
    "              'classifiers__randomforestclassifier__min_samples_leaf': [3],\n",
    "              'classifiers__randomforestclassifier__max_features': ['auto'],\n",
    "              'classifiers__randomforestclassifier__max_depth':[10],\n",
    "             'classifiers__xgbclassifier__max_depth': [3],\n",
    "             'classifiers__xgbclassifier__min_child_weight': [1],\n",
    "             'classifiers__xgbclassifier__gamma': [0.1],\n",
    "             'classifiers__xgbclassifier__colsample_bytree': [i/20.0 for i in range(13,16)],\n",
    "             'classifiers__xgbclassifier__subsample': [i/20.0 for i in range(11,14)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8398876404494382,\n",
       " {'classifiers__randomforestclassifier__max_depth': 10,\n",
       "  'classifiers__randomforestclassifier__max_features': 'auto',\n",
       "  'classifiers__randomforestclassifier__min_samples_leaf': 3,\n",
       "  'classifiers__randomforestclassifier__min_samples_split': 2,\n",
       "  'classifiers__randomforestclassifier__n_estimators': 30,\n",
       "  'classifiers__xgbclassifier__colsample_bytree': 0.7,\n",
       "  'classifiers__xgbclassifier__gamma': 0.1,\n",
       "  'classifiers__xgbclassifier__max_depth': 3,\n",
       "  'classifiers__xgbclassifier__min_child_weight': 1,\n",
       "  'classifiers__xgbclassifier__subsample': 0.6})"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteration 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__randomforestclassifier__n_estimators':[30],\n",
    "              'classifiers__randomforestclassifier__min_samples_split': [2],\n",
    "              'classifiers__randomforestclassifier__min_samples_leaf': [3],\n",
    "              'classifiers__randomforestclassifier__max_features': ['auto'],\n",
    "              'classifiers__randomforestclassifier__max_depth':[10],\n",
    "             'classifiers__xgbclassifier__max_depth': [3],\n",
    "             'classifiers__xgbclassifier__min_child_weight': [1],\n",
    "             'classifiers__xgbclassifier__gamma': [0.1],\n",
    "             'classifiers__xgbclassifier__colsample_bytree': [0.7],\n",
    "             'classifiers__xgbclassifier__subsample': [0.6],\n",
    "             'classifiers__xgbclassifier__reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8398876404494382,\n",
       " {'classifiers__randomforestclassifier__max_depth': 10,\n",
       "  'classifiers__randomforestclassifier__max_features': 'auto',\n",
       "  'classifiers__randomforestclassifier__min_samples_leaf': 3,\n",
       "  'classifiers__randomforestclassifier__min_samples_split': 2,\n",
       "  'classifiers__randomforestclassifier__n_estimators': 30,\n",
       "  'classifiers__xgbclassifier__colsample_bytree': 0.7,\n",
       "  'classifiers__xgbclassifier__gamma': 0.1,\n",
       "  'classifiers__xgbclassifier__max_depth': 3,\n",
       "  'classifiers__xgbclassifier__min_child_weight': 1,\n",
       "  'classifiers__xgbclassifier__reg_alpha': 1e-05,\n",
       "  'classifiers__xgbclassifier__subsample': 0.6})"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Meta Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params= {'classifiers__randomforestclassifier__n_estimators':[30],\n",
    "              'classifiers__randomforestclassifier__min_samples_split': [2],\n",
    "              'classifiers__randomforestclassifier__min_samples_leaf': [3],\n",
    "              'classifiers__randomforestclassifier__max_features': ['auto'],\n",
    "              'classifiers__randomforestclassifier__max_depth':[10],\n",
    "             'classifiers__xgbclassifier__max_depth': [3],\n",
    "             'classifiers__xgbclassifier__min_child_weight': [1],\n",
    "             'classifiers__xgbclassifier__gamma': [0.1],\n",
    "             'classifiers__xgbclassifier__colsample_bytree': [0.7],\n",
    "             'classifiers__xgbclassifier__subsample': [0.6],\n",
    "             'classifiers__xgbclassifier__reg_alpha': [1e-5],\n",
    "            'classifiers__meta_classifier__C': [1]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8398876404494382,\n",
       " {'classifiers__meta_classifier__C': 1,\n",
       "  'classifiers__randomforestclassifier__max_depth': 10,\n",
       "  'classifiers__randomforestclassifier__max_features': 'auto',\n",
       "  'classifiers__randomforestclassifier__min_samples_leaf': 3,\n",
       "  'classifiers__randomforestclassifier__min_samples_split': 2,\n",
       "  'classifiers__randomforestclassifier__n_estimators': 30,\n",
       "  'classifiers__xgbclassifier__colsample_bytree': 0.7,\n",
       "  'classifiers__xgbclassifier__gamma': 0.1,\n",
       "  'classifiers__xgbclassifier__max_depth': 3,\n",
       "  'classifiers__xgbclassifier__min_child_weight': 1,\n",
       "  'classifiers__xgbclassifier__reg_alpha': 1e-05,\n",
       "  'classifiers__xgbclassifier__subsample': 0.6})"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_searchCV = GridSearchCV(estimator=full_pipeline, param_grid=new_params, cv=5, scoring='accuracy')\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "grid_searchCV.fit(X_train, y_train);\n",
    "\n",
    "grid_searchCV.best_score_, grid_searchCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validated Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score: 0.8547486033519553\n",
      "Test Precision score: 0.8524590163934426\n",
      "Test Recall score: 0.7536231884057971\n",
      "Test ROC score: 0.8359025032938077\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_searchCV.predict(X_val)\n",
    "\n",
    "print(\"Test Accuracy score: \" + str(accuracy_score(y_val,y_pred)))\n",
    "print(\"Test Precision score: \" + str(precision_score(y_val,y_pred)))\n",
    "print(\"Test Recall score: \" + str(recall_score(y_val,y_pred)))\n",
    "print(\"Test ROC score: \" + str(roc_auc_score(y_val,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:portfolio_machinelearning]",
   "language": "python",
   "name": "conda-env-portfolio_machinelearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
